{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input trajectory shape: (8, 6)\n",
      "Number of environmental timesteps: 8\n",
      "\n",
      "One environmental data point contains:\n",
      "  wind fields: ['u_300', 'v_300', 'u_500', 'v_500', 'u_700', 'v_700', 'u_850', 'v_850']\n",
      "    u_300 shape: (21, 21)\n",
      "  sst shape: (21, 21)\n",
      "  geopotential shape: (21, 41)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load one of your processed samples\n",
    "with open('Processed_Data_Subset/processed_samples_1980.pkl', 'rb') as f:\n",
    "    samples = pickle.load(f)\n",
    "\n",
    "sample = samples[0]  # Look at first sample\n",
    "\n",
    "print(\"Input trajectory shape:\", sample['input_trajectory'].shape)\n",
    "print(\"Number of environmental timesteps:\", len(sample['environmental_data']))\n",
    "print(\"\\nOne environmental data point contains:\")\n",
    "for key in sample['environmental_data'][0].keys():\n",
    "    if key == 'wind':\n",
    "        print(f\"  wind fields: {list(sample['environmental_data'][0]['wind'].keys())}\")\n",
    "        print(f\"    u_300 shape: {sample['environmental_data'][0]['wind']['u_300'].shape}\")\n",
    "    elif key == 'sst':\n",
    "        print(f\"  sst shape: {sample['environmental_data'][0]['sst'].shape}\")\n",
    "    elif key == 'geopotential':\n",
    "        print(f\"  geopotential shape: {sample['environmental_data'][0]['geopotential'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Diffusion and Embedding (Same as vanilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbedding(nn.Module):\n",
    "    \"\"\"Timestep embedding for diffusion process.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = t[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion:\n",
    "    \"\"\"\n",
    "    Simplified DDPM for storm track forecasting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, timesteps=1000, beta_start=0.0001, beta_end=0.02):\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        # Linear beta schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        # Calculations for diffusion q(x_t | x_{t-1})\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        \n",
    "        # Calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = (\n",
    "            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "    \n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion: add noise to clean data.\n",
    "        \n",
    "        Args:\n",
    "            x_start: (batch, 5, 2) - clean future positions\n",
    "            t: (batch,) - diffusion timestep\n",
    "            noise: optional noise to add\n",
    "        \n",
    "        Returns:\n",
    "            x_t: noisy version of x_start at timestep t\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        \n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[t]\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t]\n",
    "        \n",
    "        # Reshape for broadcasting\n",
    "        sqrt_alpha = sqrt_alpha[:, None, None]\n",
    "        sqrt_one_minus_alpha = sqrt_one_minus_alpha[:, None, None]\n",
    "        \n",
    "        return sqrt_alpha * x_start + sqrt_one_minus_alpha * noise\n",
    "    \n",
    "    def p_sample(self, model, x_t, t, past_traj, era5_features):\n",
    "        \"\"\"\n",
    "        Reverse diffusion: denoise one step.\n",
    "        \n",
    "        Args:\n",
    "            model: DiffusionTransformer\n",
    "            x_t: (batch, 5, 2) - noisy positions at timestep t\n",
    "            t: (batch,) - current timestep\n",
    "            past_traj: (batch, 8, 6) - conditioning\n",
    "            era5_features: (batch, 8, 10) - conditioning\n",
    "        \n",
    "        Returns:\n",
    "            x_{t-1}: less noisy positions\n",
    "        \"\"\"\n",
    "        # Predict noise\n",
    "        predicted_noise = model(past_traj, era5_features, x_t, t)\n",
    "        \n",
    "        # Calculate x_0 prediction\n",
    "        alpha = self.alphas_cumprod[t][:, None, None]\n",
    "        alpha_prev = self.alphas_cumprod_prev[t][:, None, None]\n",
    "        beta = self.betas[t][:, None, None]\n",
    "        \n",
    "        # Predict x_0\n",
    "        pred_x0 = (x_t - torch.sqrt(1 - alpha) * predicted_noise) / torch.sqrt(alpha)\n",
    "        \n",
    "        # Calculate x_{t-1}\n",
    "        mean = (\n",
    "            torch.sqrt(alpha_prev) * beta * pred_x0 +\n",
    "            torch.sqrt(self.alphas[t][:, None, None]) * (1 - alpha_prev) * x_t\n",
    "        ) / (1 - alpha)\n",
    "        \n",
    "        if t[0] > 0:\n",
    "            noise = torch.randn_like(x_t)\n",
    "            variance = self.posterior_variance[t][:, None, None]\n",
    "            return mean + torch.sqrt(variance) * noise\n",
    "        else:\n",
    "            return mean\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, past_traj, era5_features, device):\n",
    "        \"\"\"\n",
    "        Generate storm track by denoising from pure noise.\n",
    "        \n",
    "        Args:\n",
    "            model: trained DiffusionTransformer\n",
    "            past_traj: (batch, 8, 6)\n",
    "            era5_features: (batch, 8, 10)\n",
    "        \n",
    "        Returns:\n",
    "            predicted_track: (batch, 5, 2) - forecasted positions\n",
    "        \"\"\"\n",
    "        batch_size = past_traj.shape[0]\n",
    "        \n",
    "        # Start from pure noise\n",
    "        x = torch.randn(batch_size, 5, 2, device=device)\n",
    "        \n",
    "        # Iteratively denoise\n",
    "        for i in reversed(range(self.timesteps)):\n",
    "            t = torch.full((batch_size,), i, device=device, dtype=torch.long)\n",
    "            x = self.p_sample(model, x, t, past_traj, era5_features)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Based ERA5 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN Encoder for ERA5 Spatial Fields\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ERA5CNNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode ERA5 spatial grids using CNN.\n",
    "    \n",
    "    Takes multiple 2D fields and produces a fixed-size embedding vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Separate encoders for different field types (different spatial sizes)\n",
    "        \n",
    "        # Wind + SST encoder (21x21 grids)\n",
    "        # Input: 9 channels (8 wind + 1 SST)\n",
    "        self.wind_sst_encoder = nn.Sequential(\n",
    "            nn.Conv2d(9, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 21x21 -> 10x10\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 10x10 -> 5x5\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),  # 5x5 -> 1x1\n",
    "        )\n",
    "        \n",
    "        # Geopotential encoder (21x41 grids)\n",
    "        # Input: 1 channel\n",
    "        self.geo_encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 21x41 -> 10x20\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 10x20 -> 5x10\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),  # 5x10 -> 1x1\n",
    "        )\n",
    "        \n",
    "        # Combine features and project to output dimension\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(128 + 64, output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, env_data_batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env_data_batch: List of environmental data (batch_size samples)\n",
    "            Each sample contains 8 timesteps of ERA5 fields\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, n_timesteps, output_dim)\n",
    "        \"\"\"\n",
    "        batch_features = []\n",
    "        \n",
    "        for env_timesteps in env_data_batch:  # Each sample in batch\n",
    "            timestep_features = []\n",
    "            \n",
    "            for env in env_timesteps:  # Each of 8 timesteps\n",
    "                # Prepare wind + SST (9 channels, 21x21)\n",
    "                wind_sst_fields = []\n",
    "                for level in [300, 500, 700, 850]:\n",
    "                    wind_sst_fields.append(env['wind'][f'u_{level}'])\n",
    "                    wind_sst_fields.append(env['wind'][f'v_{level}'])\n",
    "                wind_sst_fields.append(env['sst'])\n",
    "                \n",
    "                wind_sst = np.stack(wind_sst_fields, axis=0)  # (9, 21, 21)\n",
    "                \n",
    "                # Handle NaN values (replace with mean)\n",
    "                for i in range(wind_sst.shape[0]):\n",
    "                    field = wind_sst[i]\n",
    "                    if np.any(np.isnan(field)):\n",
    "                        wind_sst[i] = np.nan_to_num(field, nan=np.nanmean(field))\n",
    "                \n",
    "                wind_sst = torch.FloatTensor(wind_sst).unsqueeze(0)  # (1, 9, 21, 21)\n",
    "                \n",
    "                # Prepare geopotential (1 channel, 21x41)\n",
    "                geo = env['geopotential']\n",
    "                if np.any(np.isnan(geo)):\n",
    "                    geo = np.nan_to_num(geo, nan=np.nanmean(geo))\n",
    "                geo = torch.FloatTensor(geo).unsqueeze(0).unsqueeze(0)  # (1, 1, 21, 41)\n",
    "                \n",
    "                # Encode\n",
    "                wind_sst_feat = self.wind_sst_encoder(wind_sst).squeeze(-1).squeeze(-1)  # (1, 128)\n",
    "                geo_feat = self.geo_encoder(geo).squeeze(-1).squeeze(-1)  # (1, 64)\n",
    "                \n",
    "                # Fuse\n",
    "                combined = torch.cat([wind_sst_feat, geo_feat], dim=1)  # (1, 192)\n",
    "                fused = self.fusion(combined)  # (1, output_dim)\n",
    "                \n",
    "                timestep_features.append(fused)\n",
    "            \n",
    "            # Stack timesteps\n",
    "            timestep_features = torch.cat(timestep_features, dim=0)  # (8, output_dim)\n",
    "            batch_features.append(timestep_features)\n",
    "        \n",
    "        return torch.stack(batch_features, dim=0)  # (batch, 8, output_dim)\n",
    "\n",
    "\n",
    "class DiffusionTransformerCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Updated Diffusion Transformer using CNN encoder for ERA5.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=256,\n",
    "        n_heads=8,\n",
    "        n_layers=6,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # CNN encoder for ERA5\n",
    "        self.era5_encoder = ERA5CNNEncoder(output_dim=d_model)\n",
    "        \n",
    "        # Embeddings\n",
    "        self.traj_embed = nn.Linear(6, d_model)\n",
    "        self.pos_embed = nn.Linear(2, d_model)\n",
    "        \n",
    "        # Diffusion timestep embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalPositionEmbedding(d_model),\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        \n",
    "        # Transformer encoder for conditioning\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.condition_encoder = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        \n",
    "        # Transformer decoder for denoising\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.denoiser = nn.TransformerDecoder(decoder_layer, n_layers)\n",
    "        \n",
    "        # Output head\n",
    "        self.output_head = nn.Linear(d_model, 2)\n",
    "        \n",
    "        # Learnable positional encoding\n",
    "        self.forecast_pos_embed = nn.Parameter(torch.randn(5, d_model))\n",
    "    \n",
    "    def forward(self, past_traj, env_data_batch, noisy_positions, diffusion_t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            past_traj: (batch, 8, 6)\n",
    "            env_data_batch: List of environmental data dicts\n",
    "            noisy_positions: (batch, 5, 2)\n",
    "            diffusion_t: (batch,)\n",
    "        \"\"\"\n",
    "        batch_size = past_traj.shape[0]\n",
    "        \n",
    "        # Encode ERA5 with CNN\n",
    "        era5_tokens = self.era5_encoder(env_data_batch)  # (batch, 8, d_model)\n",
    "        \n",
    "        # Embed trajectory\n",
    "        traj_tokens = self.traj_embed(past_traj)  # (batch, 8, d_model)\n",
    "        \n",
    "        # Concatenate conditioning\n",
    "        conditioning = torch.cat([traj_tokens, era5_tokens], dim=1)  # (batch, 16, d_model)\n",
    "        conditioning = self.condition_encoder(conditioning)\n",
    "        \n",
    "        # Embed noisy positions\n",
    "        pos_tokens = self.pos_embed(noisy_positions)  # (batch, 5, d_model)\n",
    "        pos_tokens = pos_tokens + self.forecast_pos_embed.unsqueeze(0)\n",
    "        \n",
    "        # Add diffusion timestep\n",
    "        t_embed = self.time_embed(diffusion_t)\n",
    "        pos_tokens = pos_tokens + t_embed.unsqueeze(1)\n",
    "        \n",
    "        # Denoise\n",
    "        denoised = self.denoiser(pos_tokens, conditioning)\n",
    "        predicted_noise = self.output_head(denoised)\n",
    "        \n",
    "        return predicted_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StormDatasetCNN(Dataset):\n",
    "    \"\"\"Dataset that keeps raw ERA5 grids for CNN processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, pkl_file):\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            self.samples = pickle.load(f)\n",
    "        \n",
    "        # Filter valid samples\n",
    "        self.samples = [\n",
    "            s for s in self.samples \n",
    "            if all(s['targets'][f't+{fh}h'] is not None for fh in [6, 12, 24, 48, 72])\n",
    "        ]\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} valid samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Past trajectory\n",
    "        traj = torch.FloatTensor(sample['input_trajectory'])\n",
    "        \n",
    "        # ERA5 raw data (keep as-is for CNN)\n",
    "        env_data = sample['environmental_data']\n",
    "        \n",
    "        # Target positions\n",
    "        targets = []\n",
    "        for fh in [6, 12, 24, 48, 72]:\n",
    "            t = sample['targets'][f't+{fh}h']\n",
    "            targets.append([t['lat'], t['lon']])\n",
    "        targets = torch.FloatTensor(targets)\n",
    "        \n",
    "        return traj, env_data, targets\n",
    "\n",
    "\n",
    "def collate_fn_cnn(batch):\n",
    "    \"\"\"Custom collate to handle list of dicts in ERA5 data.\"\"\"\n",
    "    trajs, envs, targets = zip(*batch)\n",
    "    \n",
    "    trajs = torch.stack(trajs)\n",
    "    targets = torch.stack(targets)\n",
    "    # envs stays as list of lists of dicts\n",
    "    \n",
    "    return trajs, list(envs), targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_cnn(model, diffusion, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (traj, env_data, targets) in enumerate(dataloader):\n",
    "        traj = traj.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        batch_size = traj.shape[0]\n",
    "        \n",
    "        # Sample diffusion timesteps\n",
    "        t = torch.randint(0, diffusion.timesteps, (batch_size,), device=device)\n",
    "        \n",
    "        # Add noise\n",
    "        noise = torch.randn_like(targets)\n",
    "        noisy_targets = diffusion.q_sample(targets, t, noise=noise)\n",
    "        \n",
    "        # Predict noise (env_data processed inside model)\n",
    "        predicted_noise = model(traj, env_data, noisy_targets, t)\n",
    "        \n",
    "        # Loss\n",
    "        loss = nn.MSELoss()(predicted_noise, noise)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 727 valid samples\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SinusoidalPositionEmbedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m      6\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m      7\u001b[0m     dataset, \n\u001b[1;32m      8\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,  \u001b[38;5;66;03m# Smaller batch due to CNN memory\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn_cnn\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Create model with CNN encoder\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDiffusionTransformerCNN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e6\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Diffusion\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 141\u001b[0m, in \u001b[0;36mDiffusionTransformerCNN.__init__\u001b[0;34m(self, d_model, n_heads, n_layers, dropout)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m2\u001b[39m, d_model)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Diffusion timestep embedding\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_embed \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mSinusoidalPositionEmbedding\u001b[49m(d_model),\n\u001b[1;32m    142\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(d_model, d_model \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m    143\u001b[0m     nn\u001b[38;5;241m.\u001b[39mGELU(),\n\u001b[1;32m    144\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(d_model \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, d_model)\n\u001b[1;32m    145\u001b[0m )\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Transformer encoder for conditioning\u001b[39;00m\n\u001b[1;32m    148\u001b[0m encoder_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformerEncoderLayer(\n\u001b[1;32m    149\u001b[0m     d_model\u001b[38;5;241m=\u001b[39md_model,\n\u001b[1;32m    150\u001b[0m     nhead\u001b[38;5;241m=\u001b[39mn_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    154\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SinusoidalPositionEmbedding' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load data with CNN dataset\n",
    "dataset = StormDatasetCNN('Processed_Data_Subset/processed_samples_1980.pkl')\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=16,  # Smaller batch due to CNN memory\n",
    "    shuffle=True, \n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn_cnn\n",
    ")\n",
    "\n",
    "# Create model with CNN encoder\n",
    "model = DiffusionTransformerCNN(\n",
    "    d_model=256,\n",
    "    n_heads=8,\n",
    "    n_layers=6,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "\n",
    "# Diffusion\n",
    "diffusion = GaussianDiffusion(timesteps=1000)\n",
    "diffusion.betas = diffusion.betas.to(device)\n",
    "diffusion.alphas_cumprod = diffusion.alphas_cumprod.to(device)\n",
    "diffusion.alphas_cumprod_prev = diffusion.alphas_cumprod_prev.to(device)\n",
    "diffusion.sqrt_alphas_cumprod = diffusion.sqrt_alphas_cumprod.to(device)\n",
    "diffusion.sqrt_one_minus_alphas_cumprod = diffusion.sqrt_one_minus_alphas_cumprod.to(device)\n",
    "\n",
    "# Optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "# Training\n",
    "n_epochs = 100\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{n_epochs}\")\n",
    "    avg_loss = train_epoch_cnn(model, diffusion, dataloader, optimizer, device)\n",
    "    print(f\"Average loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, 'best_model_cnn.pt')\n",
    "        print(f\"âœ“ Saved best model (loss: {avg_loss:.4f})\")\n",
    "    \n",
    "    # Regular checkpoints\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, f'checkpoint_cnn_epoch_{epoch+1}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
