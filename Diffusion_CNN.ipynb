{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input trajectory shape: (8, 6)\n",
      "Number of environmental timesteps: 8\n",
      "\n",
      "One environmental data point contains:\n",
      "  wind fields: ['u_300', 'v_300', 'u_500', 'v_500', 'u_700', 'v_700', 'u_850', 'v_850']\n",
      "    u_300 shape: (21, 21)\n",
      "  sst shape: (21, 21)\n",
      "  geopotential shape: (21, 41)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load one of your processed samples\n",
    "with open('Processed_Data_Subset/processed_samples_1980.pkl', 'rb') as f:\n",
    "    samples = pickle.load(f)\n",
    "\n",
    "sample = samples[0]  # Look at first sample\n",
    "\n",
    "print(\"Input trajectory shape:\", sample['input_trajectory'].shape)\n",
    "print(\"Number of environmental timesteps:\", len(sample['environmental_data']))\n",
    "print(\"\\nOne environmental data point contains:\")\n",
    "for key in sample['environmental_data'][0].keys():\n",
    "    if key == 'wind':\n",
    "        print(f\"  wind fields: {list(sample['environmental_data'][0]['wind'].keys())}\")\n",
    "        print(f\"    u_300 shape: {sample['environmental_data'][0]['wind']['u_300'].shape}\")\n",
    "    elif key == 'sst':\n",
    "        print(f\"  sst shape: {sample['environmental_data'][0]['sst'].shape}\")\n",
    "    elif key == 'geopotential':\n",
    "        print(f\"  geopotential shape: {sample['environmental_data'][0]['geopotential'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Diffusion and Embedding (Same as vanilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbedding(nn.Module):\n",
    "    \"\"\"Timestep embedding for diffusion process.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = t[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion:\n",
    "    \"\"\"\n",
    "    Simplified DDPM for storm track forecasting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, timesteps=1000, beta_start=0.0001, beta_end=0.02):\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        # Linear beta schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        # Calculations for diffusion q(x_t | x_{t-1})\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        \n",
    "        # Calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = (\n",
    "            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "    \n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion: add noise to clean data.\n",
    "        \n",
    "        Args:\n",
    "            x_start: (batch, 5, 2) - clean future positions\n",
    "            t: (batch,) - diffusion timestep\n",
    "            noise: optional noise to add\n",
    "        \n",
    "        Returns:\n",
    "            x_t: noisy version of x_start at timestep t\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        \n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[t]\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alphas_cumprod[t]\n",
    "        \n",
    "        # Reshape for broadcasting\n",
    "        sqrt_alpha = sqrt_alpha[:, None, None]\n",
    "        sqrt_one_minus_alpha = sqrt_one_minus_alpha[:, None, None]\n",
    "        \n",
    "        return sqrt_alpha * x_start + sqrt_one_minus_alpha * noise\n",
    "    \n",
    "    def p_sample(self, model, x_t, t, past_traj, era5_features):\n",
    "        \"\"\"\n",
    "        Reverse diffusion: denoise one step.\n",
    "        \n",
    "        Args:\n",
    "            model: DiffusionTransformer\n",
    "            x_t: (batch, 5, 2) - noisy positions at timestep t\n",
    "            t: (batch,) - current timestep\n",
    "            past_traj: (batch, 8, 6) - conditioning\n",
    "            era5_features: (batch, 8, 10) - conditioning\n",
    "        \n",
    "        Returns:\n",
    "            x_{t-1}: less noisy positions\n",
    "        \"\"\"\n",
    "        # Predict noise\n",
    "        predicted_noise = model(past_traj, era5_features, x_t, t)\n",
    "        \n",
    "        # Calculate x_0 prediction\n",
    "        alpha = self.alphas_cumprod[t][:, None, None]\n",
    "        alpha_prev = self.alphas_cumprod_prev[t][:, None, None]\n",
    "        beta = self.betas[t][:, None, None]\n",
    "        \n",
    "        # Predict x_0\n",
    "        pred_x0 = (x_t - torch.sqrt(1 - alpha) * predicted_noise) / torch.sqrt(alpha)\n",
    "        \n",
    "        # Calculate x_{t-1}\n",
    "        mean = (\n",
    "            torch.sqrt(alpha_prev) * beta * pred_x0 +\n",
    "            torch.sqrt(self.alphas[t][:, None, None]) * (1 - alpha_prev) * x_t\n",
    "        ) / (1 - alpha)\n",
    "        \n",
    "        if t[0] > 0:\n",
    "            noise = torch.randn_like(x_t)\n",
    "            variance = self.posterior_variance[t][:, None, None]\n",
    "            return mean + torch.sqrt(variance) * noise\n",
    "        else:\n",
    "            return mean\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, past_traj, era5_features, device):\n",
    "        \"\"\"\n",
    "        Generate storm track by denoising from pure noise.\n",
    "        \n",
    "        Args:\n",
    "            model: trained DiffusionTransformer\n",
    "            past_traj: (batch, 8, 6)\n",
    "            era5_features: (batch, 8, 10)\n",
    "        \n",
    "        Returns:\n",
    "            predicted_track: (batch, 5, 2) - forecasted positions\n",
    "        \"\"\"\n",
    "        batch_size = past_traj.shape[0]\n",
    "        \n",
    "        # Start from pure noise\n",
    "        x = torch.randn(batch_size, 5, 2, device=device)\n",
    "        \n",
    "        # Iteratively denoise\n",
    "        for i in reversed(range(self.timesteps)):\n",
    "            t = torch.full((batch_size,), i, device=device, dtype=torch.long)\n",
    "            x = self.p_sample(model, x, t, past_traj, era5_features)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Based ERA5 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN Encoder for ERA5 Spatial Fields\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ERA5CNNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode ERA5 spatial grids using CNN.\n",
    "    \n",
    "    Takes multiple 2D fields and produces a fixed-size embedding vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Separate encoders for different field types (different spatial sizes)\n",
    "        \n",
    "        # Wind + SST encoder (21x21 grids)\n",
    "        # Input: 9 channels (8 wind + 1 SST)\n",
    "        self.wind_sst_encoder = nn.Sequential(\n",
    "            nn.Conv2d(9, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 21x21 -> 10x10\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 10x10 -> 5x5\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),  # 5x5 -> 1x1\n",
    "        )\n",
    "        \n",
    "        # Geopotential encoder (21x41 grids)\n",
    "        # Input: 1 channel\n",
    "        self.geo_encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 21x41 -> 10x20\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 10x20 -> 5x10\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),  # 5x10 -> 1x1\n",
    "        )\n",
    "        \n",
    "        # Combine features and project to output dimension\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(128 + 64, output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, env_data_batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env_data_batch: List of environmental data (batch_size samples)\n",
    "            Each sample contains 8 timesteps of ERA5 fields\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, n_timesteps, output_dim)\n",
    "        \"\"\"\n",
    "        batch_features = []\n",
    "        \n",
    "        for env_timesteps in env_data_batch:  # Each sample in batch\n",
    "            timestep_features = []\n",
    "            \n",
    "            for env in env_timesteps:  # Each of 8 timesteps\n",
    "                # Prepare wind + SST (9 channels, 21x21)\n",
    "                wind_sst_fields = []\n",
    "                for level in [300, 500, 700, 850]:\n",
    "                    wind_sst_fields.append(env['wind'][f'u_{level}'])\n",
    "                    wind_sst_fields.append(env['wind'][f'v_{level}'])\n",
    "                wind_sst_fields.append(env['sst'])\n",
    "                \n",
    "                wind_sst = np.stack(wind_sst_fields, axis=0)  # (9, 21, 21)\n",
    "                \n",
    "                # Handle NaN values (replace with mean)\n",
    "                for i in range(wind_sst.shape[0]):\n",
    "                    field = wind_sst[i]\n",
    "                    if np.any(np.isnan(field)):\n",
    "                        wind_sst[i] = np.nan_to_num(field, nan=np.nanmean(field))\n",
    "                \n",
    "                wind_sst = torch.FloatTensor(wind_sst).unsqueeze(0)  # (1, 9, 21, 21)\n",
    "                \n",
    "                # Prepare geopotential (1 channel, 21x41)\n",
    "                geo = env['geopotential']\n",
    "                if np.any(np.isnan(geo)):\n",
    "                    geo = np.nan_to_num(geo, nan=np.nanmean(geo))\n",
    "                geo = torch.FloatTensor(geo).unsqueeze(0).unsqueeze(0)  # (1, 1, 21, 41)\n",
    "                \n",
    "                # Encode\n",
    "                wind_sst_feat = self.wind_sst_encoder(wind_sst).squeeze(-1).squeeze(-1)  # (1, 128)\n",
    "                geo_feat = self.geo_encoder(geo).squeeze(-1).squeeze(-1)  # (1, 64)\n",
    "                \n",
    "                # Fuse\n",
    "                combined = torch.cat([wind_sst_feat, geo_feat], dim=1)  # (1, 192)\n",
    "                fused = self.fusion(combined)  # (1, output_dim)\n",
    "                \n",
    "                timestep_features.append(fused)\n",
    "            \n",
    "            # Stack timesteps\n",
    "            timestep_features = torch.cat(timestep_features, dim=0)  # (8, output_dim)\n",
    "            batch_features.append(timestep_features)\n",
    "        \n",
    "        return torch.stack(batch_features, dim=0)  # (batch, 8, output_dim)\n",
    "\n",
    "\n",
    "class DiffusionTransformerCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Updated Diffusion Transformer using CNN encoder for ERA5.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=256,\n",
    "        n_heads=8,\n",
    "        n_layers=6,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # CNN encoder for ERA5\n",
    "        self.era5_encoder = ERA5CNNEncoder(output_dim=d_model)\n",
    "        \n",
    "        # Embeddings\n",
    "        self.traj_embed = nn.Linear(6, d_model)\n",
    "        self.pos_embed = nn.Linear(2, d_model)\n",
    "        \n",
    "        # Diffusion timestep embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalPositionEmbedding(d_model),\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        \n",
    "        # Transformer encoder for conditioning\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.condition_encoder = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        \n",
    "        # Transformer decoder for denoising\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.denoiser = nn.TransformerDecoder(decoder_layer, n_layers)\n",
    "        \n",
    "        # Output head\n",
    "        self.output_head = nn.Linear(d_model, 2)\n",
    "        \n",
    "        # Learnable positional encoding\n",
    "        self.forecast_pos_embed = nn.Parameter(torch.randn(5, d_model))\n",
    "    \n",
    "    def forward(self, past_traj, env_data_batch, noisy_positions, diffusion_t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            past_traj: (batch, 8, 6)\n",
    "            env_data_batch: List of environmental data dicts\n",
    "            noisy_positions: (batch, 5, 2)\n",
    "            diffusion_t: (batch,)\n",
    "        \"\"\"\n",
    "        batch_size = past_traj.shape[0]\n",
    "        \n",
    "        # Encode ERA5 with CNN\n",
    "        era5_tokens = self.era5_encoder(env_data_batch)  # (batch, 8, d_model)\n",
    "        \n",
    "        # Embed trajectory\n",
    "        traj_tokens = self.traj_embed(past_traj)  # (batch, 8, d_model)\n",
    "        \n",
    "        # Concatenate conditioning\n",
    "        conditioning = torch.cat([traj_tokens, era5_tokens], dim=1)  # (batch, 16, d_model)\n",
    "        conditioning = self.condition_encoder(conditioning)\n",
    "        \n",
    "        # Embed noisy positions\n",
    "        pos_tokens = self.pos_embed(noisy_positions)  # (batch, 5, d_model)\n",
    "        pos_tokens = pos_tokens + self.forecast_pos_embed.unsqueeze(0)\n",
    "        \n",
    "        # Add diffusion timestep\n",
    "        t_embed = self.time_embed(diffusion_t)\n",
    "        pos_tokens = pos_tokens + t_embed.unsqueeze(1)\n",
    "        \n",
    "        # Denoise\n",
    "        denoised = self.denoiser(pos_tokens, conditioning)\n",
    "        predicted_noise = self.output_head(denoised)\n",
    "        \n",
    "        return predicted_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StormDatasetCNN(Dataset):\n",
    "    \"\"\"Dataset that keeps raw ERA5 grids for CNN processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, pkl_file):\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            self.samples = pickle.load(f)\n",
    "        \n",
    "        # Filter valid samples\n",
    "        self.samples = [\n",
    "            s for s in self.samples \n",
    "            if all(s['targets'][f't+{fh}h'] is not None for fh in [6, 12, 24, 48, 72])\n",
    "        ]\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} valid samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Past trajectory\n",
    "        traj = torch.FloatTensor(sample['input_trajectory'])\n",
    "        \n",
    "        # ERA5 raw data (keep as-is for CNN)\n",
    "        env_data = sample['environmental_data']\n",
    "        \n",
    "        # Target positions\n",
    "        targets = []\n",
    "        for fh in [6, 12, 24, 48, 72]:\n",
    "            t = sample['targets'][f't+{fh}h']\n",
    "            targets.append([t['lat'], t['lon']])\n",
    "        targets = torch.FloatTensor(targets)\n",
    "        \n",
    "        return traj, env_data, targets\n",
    "\n",
    "\n",
    "def collate_fn_cnn(batch):\n",
    "    \"\"\"Custom collate to handle list of dicts in ERA5 data.\"\"\"\n",
    "    trajs, envs, targets = zip(*batch)\n",
    "    \n",
    "    trajs = torch.stack(trajs)\n",
    "    targets = torch.stack(targets)\n",
    "    # envs stays as list of lists of dicts\n",
    "    \n",
    "    return trajs, list(envs), targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'd_model': 256,\n",
    "    'n_heads': 8,\n",
    "    'n_layers': 6,\n",
    "    'dropout': 0.1,\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'n_epochs': 100,\n",
    "    'grad_clip': 1.0,\n",
    "    \n",
    "    # Diffusion\n",
    "    'diffusion_timesteps': 1000,\n",
    "    \n",
    "    # Data\n",
    "    'data_path': 'Processed_Data_Subset/processed_samples_1980.pkl',\n",
    "    \n",
    "    # Logging\n",
    "    'log_interval': 10,\n",
    "    'save_interval': 20,\n",
    "    'checkpoint_dir': 'checkpoints',\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING WITH WANDB\n",
    "# ============================================================================\n",
    "\n",
    "def train_epoch_logged(model, diffusion, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    for batch_idx, (traj, env_data, targets) in enumerate(dataloader):\n",
    "        traj = traj.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        batch_size = traj.shape[0]\n",
    "        \n",
    "        # Sample diffusion timesteps\n",
    "        t = torch.randint(0, diffusion.timesteps, (batch_size,), device=device)\n",
    "        \n",
    "        # Add noise\n",
    "        noise = torch.randn_like(targets)\n",
    "        noisy_targets = diffusion.q_sample(targets, t, noise=noise)\n",
    "        \n",
    "        # Forward pass\n",
    "        predicted_noise = model(traj, env_data, noisy_targets, t)\n",
    "        \n",
    "        # Loss\n",
    "        loss = nn.MSELoss()(predicted_noise, noise)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        step += 1\n",
    "        \n",
    "        # Log to wandb\n",
    "        if batch_idx % CONFIG['log_interval'] == 0:\n",
    "            wandb.log({\n",
    "                'train/batch_loss': loss.item(),\n",
    "                'train/epoch': epoch,\n",
    "                'train/step': epoch * len(dataloader) + batch_idx,\n",
    "            })\n",
    "            print(f\"  Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, diffusion, dataset, device, n_samples=50):\n",
    "    \"\"\"Quick evaluation on a subset.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    for idx in range(min(n_samples, len(dataset))):\n",
    "        traj, env_data, actual = dataset[idx]\n",
    "        \n",
    "        # Generate prediction\n",
    "        pred = diffusion.sample(\n",
    "            model,\n",
    "            traj.unsqueeze(0).to(device),\n",
    "            [env_data],  # Wrap in list for batch\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        pred = pred[0].cpu().numpy()\n",
    "        actual = actual.numpy()\n",
    "        \n",
    "        # Calculate 72h error (last position)\n",
    "        lat_err = (pred[4, 0] - actual[4, 0]) * 111\n",
    "        lon_err = (pred[4, 1] - actual[4, 1]) * 111 * np.cos(np.radians(actual[4, 0]))\n",
    "        dist_err = np.sqrt(lat_err**2 + lon_err**2)\n",
    "        errors.append(dist_err)\n",
    "    \n",
    "    return np.mean(errors), np.std(errors)\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, loss, filename):\n",
    "    \"\"\"Save checkpoint with all training state.\"\"\"\n",
    "    checkpoint_path = Path(CONFIG['checkpoint_dir']) / filename\n",
    "    checkpoint_path.parent.mkdir(exist_ok=True)\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': loss,\n",
    "        'config': CONFIG,\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    # Also save to wandb\n",
    "    wandb.save(str(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project=\"cyclone-diffusion-transformer\",\n",
    "    config=CONFIG,\n",
    "    name=f\"cnn-encoder-{CONFIG['d_model']}d-{CONFIG['n_layers']}L\",\n",
    ")\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"  Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Load data\n",
    "print(f\"\\n Loading data...\")\n",
    "dataset = StormDatasetCNN(CONFIG['data_path'])\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn_cnn\n",
    ")\n",
    "\n",
    "# Create model\n",
    "print(f\"\\n Building model...\")\n",
    "model = DiffusionTransformerCNN(\n",
    "    d_model=CONFIG['d_model'],\n",
    "    n_heads=CONFIG['n_heads'],\n",
    "    n_layers=CONFIG['n_layers'],\n",
    "    dropout=CONFIG['dropout']\n",
    ").to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"   Parameters: {n_params/1e6:.2f}M\")\n",
    "wandb.config.update({'n_parameters': n_params})\n",
    "\n",
    "# Watch model gradients in wandb\n",
    "wandb.watch(model, log='all', log_freq=100)\n",
    "\n",
    "# Diffusion\n",
    "diffusion = GaussianDiffusion(timesteps=CONFIG['diffusion_timesteps'])\n",
    "diffusion.betas = diffusion.betas.to(device)\n",
    "diffusion.alphas_cumprod = diffusion.alphas_cumprod.to(device)\n",
    "diffusion.alphas_cumprod_prev = diffusion.alphas_cumprod_prev.to(device)\n",
    "diffusion.sqrt_alphas_cumprod = diffusion.sqrt_alphas_cumprod.to(device)\n",
    "diffusion.sqrt_one_minus_alphas_cumprod = diffusion.sqrt_one_minus_alphas_cumprod.to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=CONFIG['n_epochs']\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "print(f\"\\n Starting training for {CONFIG['n_epochs']} epochs...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(CONFIG['n_epochs']):\n",
    "    print(f\"\\n Epoch {epoch+1}/{CONFIG['n_epochs']}\")\n",
    "    \n",
    "    # Train\n",
    "    avg_loss = train_epoch_logged(model, diffusion, dataloader, optimizer, device, epoch)\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_mean, eval_std = evaluate(model, diffusion, dataset, device, n_samples=50)\n",
    "    \n",
    "    # Log metrics\n",
    "    wandb.log({\n",
    "        'train/epoch_loss': avg_loss,\n",
    "        'eval/72h_error_mean_km': eval_mean,\n",
    "        'eval/72h_error_std_km': eval_std,\n",
    "        'train/learning_rate': optimizer.param_groups[0]['lr'],\n",
    "        'epoch': epoch,\n",
    "    })\n",
    "    \n",
    "    print(f\"   Train Loss: {avg_loss:.4f}\")\n",
    "    print(f\"   Eval 72h Error: {eval_mean:.1f} ± {eval_std:.1f} km\")\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch, avg_loss, 'best_model.pt')\n",
    "        wandb.run.summary[\"best_loss\"] = best_loss\n",
    "        wandb.run.summary[\"best_epoch\"] = epoch\n",
    "    \n",
    "    # Regular checkpoints\n",
    "    if (epoch + 1) % CONFIG['save_interval'] == 0:\n",
    "        save_checkpoint(\n",
    "            model, optimizer, scheduler, epoch, avg_loss,\n",
    "            f'checkpoint_epoch_{epoch+1}.pt'\n",
    "        )\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ Training complete!\")\n",
    "print(f\"   Best loss: {best_loss:.4f}\")\n",
    "\n",
    "# Save final model\n",
    "save_checkpoint(model, optimizer, scheduler, CONFIG['n_epochs']-1, avg_loss, 'final_model.pt')\n",
    "\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
